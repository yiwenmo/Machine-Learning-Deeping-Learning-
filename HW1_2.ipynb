{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssru5Zf_TGGR"
      },
      "source": [
        "### **Problem Description**\n",
        "In the boolean_learning.ipynb we have seen that increasing the training sample will improve the testing accuracy. Now we want to see how the feature number will exponentially affect the size of training data. We change the feature number from 4 bits into 8 bits. Please try to following the guide and report at least how many training data you need to get the lowest error rate. (i.e., how many training data you need to generate at least so that the training error would converge.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am_v2V4EUjTM"
      },
      "source": [
        "### **Generate the target value**\n",
        "We first generate all the data with given target value, which is the target function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "GH17WjCgRqcI",
        "outputId": "731fec9e-9d26-481e-9ce8-201b05f85ca1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas import DataFrame\n",
        "df=DataFrame(index=pd.Index(['{0:04b}'.format(i) for i in range(2**8)],\n",
        "                            dtype='str',\n",
        "                            name='x'),columns=['f'])\n",
        "df.f=np.array(df.index.map(lambda i:i.count('0')) \n",
        "               > df.index.map(lambda i:i.count('1')),dtype=int)\n",
        "df # show all the input vectors and target values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          f\n",
              "x          \n",
              "0000      1\n",
              "0001      1\n",
              "0010      1\n",
              "0011      0\n",
              "0100      1\n",
              "...      ..\n",
              "11111011  0\n",
              "11111100  0\n",
              "11111101  0\n",
              "11111110  0\n",
              "11111111  0\n",
              "\n",
              "[256 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51a01bef-3713-4944-a27c-35df27b87069\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0000</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0001</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0010</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0011</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0100</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11111011</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11111100</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11111101</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11111110</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11111111</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>256 rows Ã— 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51a01bef-3713-4944-a27c-35df27b87069')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51a01bef-3713-4944-a27c-35df27b87069 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51a01bef-3713-4944-a27c-35df27b87069');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08YQav67VmPL"
      },
      "source": [
        "### **Sampling function**\n",
        "This get_sample function is to randomly sample the data from all 2^8=256 possible dataset. Unlike the class, we do not let possibility of sampling the first eight elements be twice of the last eight. We simply sample the data uniformly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbhk8V-RRyQ7"
      },
      "source": [
        "def get_sample(n=1):\n",
        "   if n==1:\n",
        "      return '{0:04b}'.format(np.random.choice(list(range(256))))\n",
        "   else:\n",
        "      return [get_sample(1) for _ in range(n)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p40DjGgZWn-r"
      },
      "source": [
        "### **Student ID**\n",
        "Please fill in your student IDs below. Please notice that different student IDs should receive different dataset, which will lead to different answer. Make sure you fill the correct student IDs that you enrolled this class. Otherwise, you will get wrong answer and get no points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySz_KVBTWvsS"
      },
      "source": [
        "student_id = 'r10521804' # fill with your student ID\n",
        "\n",
        "assert student_id != 'your_student_id', 'Please fill in your student_id before you start.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR_gt1IgW2_f"
      },
      "source": [
        "### **Sample the training data**\n",
        "Let's sample some data as the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE_xxNkPWx0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89dd5451-3086-4838-df0e-5ebc186331e0"
      },
      "source": [
        "np.random.seed(int(student_id[-2:])) # for reproduction\n",
        "train=df.loc[get_sample(5),'f'] # 5-element training set\n",
        "train.index.unique().shape    # how many unique elements? and we only sample one point for each unique element.\n",
        "df['g']=df.loc[train.index.unique(),'f']\n",
        "df.g"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x\n",
              "0000       NaN\n",
              "0001       NaN\n",
              "0010       NaN\n",
              "0011       NaN\n",
              "0100       NaN\n",
              "            ..\n",
              "11111011   NaN\n",
              "11111100   NaN\n",
              "11111101   NaN\n",
              "11111110   NaN\n",
              "11111111   NaN\n",
              "Name: g, Length: 256, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwuh8N8sYNDp"
      },
      "source": [
        "### **Sample the testing data**\n",
        "Let's sample some data as the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPRlV5NqYQqJ"
      },
      "source": [
        "np.random.seed(int(student_id[-2:]))\n",
        "test= df.loc[get_sample(2560),'f']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkakxu5XYnLm"
      },
      "source": [
        "### **Guessing the f**\n",
        "Now we need to use the training dataset to generate a function g as our guess of f. As we haven't taught any learning algorithm yet, we simply guess the target value as **5** when the data is outside the training dataset. By doing so, the value that's not in the training dataset will be the error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PpjaHDQZrsc"
      },
      "source": [
        "df.g.fillna(5,inplace=True) # final specification of g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBz-xjDDZvVh"
      },
      "source": [
        "### **Calculate the error rate**\n",
        "Now we have our g function and testing dataset. Let's calculate the error rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU1GJYHqZ9KG",
        "outputId": "31f2f30e-f0e3-43b7-b54f-6ef73ce8f69d"
      },
      "source": [
        "(df.loc[test.index,'g'] != df.loc[test.index,'f']).mean() # error rate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.976953125"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w2V5dHganJ3"
      },
      "source": [
        "### **TODO**\n",
        "What we want you to do is to modify the code which generate the training dataset. Please find out that how much training data you should generate to let the error 0.2. It should be quit difficult to get precisely 0.2, but make sure the norm1(0.2 - error) is lower than 0.01. Another important thing is that you should find the **least** number of training data, not just any number that can make the error less than 0.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OykDa4ZAFi1"
      },
      "source": [
        "# TODO finish this function\n",
        "######################################## hint ########################################\n",
        "# train=df.loc[get_sample(sample_num),'f'] # try to change the sample_num which change the training data number and see whate happen with the error\n",
        "# del df['g']\n",
        "# df.g.fillna(5,inplace=True) # final specification of g\n",
        "# print((df.loc[test.index,'g'] != df.loc[test.index,'f']).mean()) # error rate\n",
        "######################################################################################\n",
        "def find_sample_num(dataframe):\n",
        "  # Do not delete this line\n",
        "  np.random.seed(int(student_id[-2:]))\n",
        "  # while or something\n",
        "  sample_num = 2 #if input 1 there is an error\n",
        "  error_rate = 1 #the maximum error rate start at 1\n",
        "\n",
        "  while error_rate >= 0.2: \n",
        "    train=df.loc[get_sample(sample_num),'f']\n",
        "    df['g']=df.loc[train.index.unique(),'f']\n",
        "    df.g.fillna(5,inplace=True)\n",
        "    error_rate = (df.loc[test.index,'g'] != df.loc[test.index,'f']).mean()\n",
        "    del df['g']\n",
        "    sample_num += 1\n",
        "    print(sample_num)\n",
        "    print(error_rate)\n",
        "    \n",
        "  return sample_num-1 #run for one loop will add one to sample_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bp0N3VlBmRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd21af8c-b315-438f-9fed-9650369c85cf"
      },
      "source": [
        "# Try to find out the sample number\n",
        "sample_num = find_sample_num(df)\n",
        "sample_num"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0.9890625\n",
            "4\n",
            "0.987890625\n",
            "5\n",
            "0.984375\n",
            "6\n",
            "0.983984375\n",
            "7\n",
            "0.973046875\n",
            "8\n",
            "0.9671875\n",
            "9\n",
            "0.9671875\n",
            "10\n",
            "0.965234375\n",
            "11\n",
            "0.955859375\n",
            "12\n",
            "0.955078125\n",
            "13\n",
            "0.94921875\n",
            "14\n",
            "0.946484375\n",
            "15\n",
            "0.94296875\n",
            "16\n",
            "0.94609375\n",
            "17\n",
            "0.9375\n",
            "18\n",
            "0.934375\n",
            "19\n",
            "0.923828125\n",
            "20\n",
            "0.92265625\n",
            "21\n",
            "0.917578125\n",
            "22\n",
            "0.905859375\n",
            "23\n",
            "0.91328125\n",
            "24\n",
            "0.91328125\n",
            "25\n",
            "0.8984375\n",
            "26\n",
            "0.89921875\n",
            "27\n",
            "0.901171875\n",
            "28\n",
            "0.8953125\n",
            "29\n",
            "0.884375\n",
            "30\n",
            "0.884765625\n",
            "31\n",
            "0.880078125\n",
            "32\n",
            "0.876171875\n",
            "33\n",
            "0.876171875\n",
            "34\n",
            "0.859765625\n",
            "35\n",
            "0.86015625\n",
            "36\n",
            "0.87578125\n",
            "37\n",
            "0.85859375\n",
            "38\n",
            "0.8421875\n",
            "39\n",
            "0.844921875\n",
            "40\n",
            "0.8640625\n",
            "41\n",
            "0.840234375\n",
            "42\n",
            "0.842578125\n",
            "43\n",
            "0.828515625\n",
            "44\n",
            "0.830859375\n",
            "45\n",
            "0.826171875\n",
            "46\n",
            "0.80859375\n",
            "47\n",
            "0.82578125\n",
            "48\n",
            "0.819140625\n",
            "49\n",
            "0.808203125\n",
            "50\n",
            "0.791796875\n",
            "51\n",
            "0.814453125\n",
            "52\n",
            "0.81484375\n",
            "53\n",
            "0.7984375\n",
            "54\n",
            "0.798828125\n",
            "55\n",
            "0.78203125\n",
            "56\n",
            "0.774609375\n",
            "57\n",
            "0.768359375\n",
            "58\n",
            "0.79375\n",
            "59\n",
            "0.76484375\n",
            "60\n",
            "0.776953125\n",
            "61\n",
            "0.81484375\n",
            "62\n",
            "0.75859375\n",
            "63\n",
            "0.757421875\n",
            "64\n",
            "0.762890625\n",
            "65\n",
            "0.743359375\n",
            "66\n",
            "0.750390625\n",
            "67\n",
            "0.768359375\n",
            "68\n",
            "0.753125\n",
            "69\n",
            "0.7359375\n",
            "70\n",
            "0.766796875\n",
            "71\n",
            "0.741015625\n",
            "72\n",
            "0.75\n",
            "73\n",
            "0.747265625\n",
            "74\n",
            "0.765234375\n",
            "75\n",
            "0.775390625\n",
            "76\n",
            "0.7546875\n",
            "77\n",
            "0.75546875\n",
            "78\n",
            "0.734765625\n",
            "79\n",
            "0.7453125\n",
            "80\n",
            "0.743359375\n",
            "81\n",
            "0.707421875\n",
            "82\n",
            "0.7359375\n",
            "83\n",
            "0.724609375\n",
            "84\n",
            "0.71875\n",
            "85\n",
            "0.717578125\n",
            "86\n",
            "0.712109375\n",
            "87\n",
            "0.7109375\n",
            "88\n",
            "0.704296875\n",
            "89\n",
            "0.723046875\n",
            "90\n",
            "0.702734375\n",
            "91\n",
            "0.6953125\n",
            "92\n",
            "0.7078125\n",
            "93\n",
            "0.69453125\n",
            "94\n",
            "0.688671875\n",
            "95\n",
            "0.696484375\n",
            "96\n",
            "0.68828125\n",
            "97\n",
            "0.71328125\n",
            "98\n",
            "0.68125\n",
            "99\n",
            "0.67265625\n",
            "100\n",
            "0.672265625\n",
            "101\n",
            "0.6828125\n",
            "102\n",
            "0.700390625\n",
            "103\n",
            "0.683203125\n",
            "104\n",
            "0.653125\n",
            "105\n",
            "0.671484375\n",
            "106\n",
            "0.66796875\n",
            "107\n",
            "0.626953125\n",
            "108\n",
            "0.657421875\n",
            "109\n",
            "0.646484375\n",
            "110\n",
            "0.65546875\n",
            "111\n",
            "0.651953125\n",
            "112\n",
            "0.6421875\n",
            "113\n",
            "0.6359375\n",
            "114\n",
            "0.621484375\n",
            "115\n",
            "0.64375\n",
            "116\n",
            "0.678515625\n",
            "117\n",
            "0.619140625\n",
            "118\n",
            "0.642578125\n",
            "119\n",
            "0.618359375\n",
            "120\n",
            "0.645703125\n",
            "121\n",
            "0.63046875\n",
            "122\n",
            "0.625\n",
            "123\n",
            "0.6125\n",
            "124\n",
            "0.62890625\n",
            "125\n",
            "0.594921875\n",
            "126\n",
            "0.58359375\n",
            "127\n",
            "0.62578125\n",
            "128\n",
            "0.626953125\n",
            "129\n",
            "0.59453125\n",
            "130\n",
            "0.605859375\n",
            "131\n",
            "0.606640625\n",
            "132\n",
            "0.596484375\n",
            "133\n",
            "0.5640625\n",
            "134\n",
            "0.61796875\n",
            "135\n",
            "0.604296875\n",
            "136\n",
            "0.6015625\n",
            "137\n",
            "0.585546875\n",
            "138\n",
            "0.600390625\n",
            "139\n",
            "0.618359375\n",
            "140\n",
            "0.59921875\n",
            "141\n",
            "0.584765625\n",
            "142\n",
            "0.574609375\n",
            "143\n",
            "0.55625\n",
            "144\n",
            "0.55625\n",
            "145\n",
            "0.575\n",
            "146\n",
            "0.60234375\n",
            "147\n",
            "0.544140625\n",
            "148\n",
            "0.5328125\n",
            "149\n",
            "0.56171875\n",
            "150\n",
            "0.555078125\n",
            "151\n",
            "0.54375\n",
            "152\n",
            "0.56484375\n",
            "153\n",
            "0.55078125\n",
            "154\n",
            "0.504296875\n",
            "155\n",
            "0.541796875\n",
            "156\n",
            "0.56875\n",
            "157\n",
            "0.5671875\n",
            "158\n",
            "0.5671875\n",
            "159\n",
            "0.55546875\n",
            "160\n",
            "0.56875\n",
            "161\n",
            "0.5328125\n",
            "162\n",
            "0.548046875\n",
            "163\n",
            "0.54453125\n",
            "164\n",
            "0.501953125\n",
            "165\n",
            "0.53125\n",
            "166\n",
            "0.50390625\n",
            "167\n",
            "0.51796875\n",
            "168\n",
            "0.52890625\n",
            "169\n",
            "0.519921875\n",
            "170\n",
            "0.553125\n",
            "171\n",
            "0.534375\n",
            "172\n",
            "0.512109375\n",
            "173\n",
            "0.5125\n",
            "174\n",
            "0.4875\n",
            "175\n",
            "0.477734375\n",
            "176\n",
            "0.5203125\n",
            "177\n",
            "0.51015625\n",
            "178\n",
            "0.50546875\n",
            "179\n",
            "0.494921875\n",
            "180\n",
            "0.5\n",
            "181\n",
            "0.476953125\n",
            "182\n",
            "0.51875\n",
            "183\n",
            "0.4828125\n",
            "184\n",
            "0.475\n",
            "185\n",
            "0.478515625\n",
            "186\n",
            "0.497265625\n",
            "187\n",
            "0.48125\n",
            "188\n",
            "0.46484375\n",
            "189\n",
            "0.507421875\n",
            "190\n",
            "0.508203125\n",
            "191\n",
            "0.440625\n",
            "192\n",
            "0.47421875\n",
            "193\n",
            "0.451171875\n",
            "194\n",
            "0.48671875\n",
            "195\n",
            "0.485546875\n",
            "196\n",
            "0.479296875\n",
            "197\n",
            "0.463671875\n",
            "198\n",
            "0.4375\n",
            "199\n",
            "0.469921875\n",
            "200\n",
            "0.476171875\n",
            "201\n",
            "0.42421875\n",
            "202\n",
            "0.437890625\n",
            "203\n",
            "0.503515625\n",
            "204\n",
            "0.473828125\n",
            "205\n",
            "0.47890625\n",
            "206\n",
            "0.4078125\n",
            "207\n",
            "0.469921875\n",
            "208\n",
            "0.45546875\n",
            "209\n",
            "0.461328125\n",
            "210\n",
            "0.471484375\n",
            "211\n",
            "0.42890625\n",
            "212\n",
            "0.441796875\n",
            "213\n",
            "0.421484375\n",
            "214\n",
            "0.42421875\n",
            "215\n",
            "0.44921875\n",
            "216\n",
            "0.473046875\n",
            "217\n",
            "0.430078125\n",
            "218\n",
            "0.416796875\n",
            "219\n",
            "0.41171875\n",
            "220\n",
            "0.433203125\n",
            "221\n",
            "0.402734375\n",
            "222\n",
            "0.442578125\n",
            "223\n",
            "0.41796875\n",
            "224\n",
            "0.427734375\n",
            "225\n",
            "0.382421875\n",
            "226\n",
            "0.421484375\n",
            "227\n",
            "0.457421875\n",
            "228\n",
            "0.423828125\n",
            "229\n",
            "0.3859375\n",
            "230\n",
            "0.4046875\n",
            "231\n",
            "0.41796875\n",
            "232\n",
            "0.45\n",
            "233\n",
            "0.4140625\n",
            "234\n",
            "0.40625\n",
            "235\n",
            "0.44921875\n",
            "236\n",
            "0.4140625\n",
            "237\n",
            "0.380078125\n",
            "238\n",
            "0.382421875\n",
            "239\n",
            "0.37734375\n",
            "240\n",
            "0.3796875\n",
            "241\n",
            "0.365625\n",
            "242\n",
            "0.391796875\n",
            "243\n",
            "0.37578125\n",
            "244\n",
            "0.35859375\n",
            "245\n",
            "0.360546875\n",
            "246\n",
            "0.378515625\n",
            "247\n",
            "0.374609375\n",
            "248\n",
            "0.38515625\n",
            "249\n",
            "0.367578125\n",
            "250\n",
            "0.39453125\n",
            "251\n",
            "0.384765625\n",
            "252\n",
            "0.3828125\n",
            "253\n",
            "0.38671875\n",
            "254\n",
            "0.407421875\n",
            "255\n",
            "0.376171875\n",
            "256\n",
            "0.3515625\n",
            "257\n",
            "0.36640625\n",
            "258\n",
            "0.362890625\n",
            "259\n",
            "0.38046875\n",
            "260\n",
            "0.3671875\n",
            "261\n",
            "0.3625\n",
            "262\n",
            "0.348046875\n",
            "263\n",
            "0.355859375\n",
            "264\n",
            "0.332421875\n",
            "265\n",
            "0.357421875\n",
            "266\n",
            "0.39921875\n",
            "267\n",
            "0.380078125\n",
            "268\n",
            "0.316015625\n",
            "269\n",
            "0.37421875\n",
            "270\n",
            "0.340625\n",
            "271\n",
            "0.351953125\n",
            "272\n",
            "0.311328125\n",
            "273\n",
            "0.27890625\n",
            "274\n",
            "0.331640625\n",
            "275\n",
            "0.33203125\n",
            "276\n",
            "0.32734375\n",
            "277\n",
            "0.334765625\n",
            "278\n",
            "0.3203125\n",
            "279\n",
            "0.337109375\n",
            "280\n",
            "0.324609375\n",
            "281\n",
            "0.3578125\n",
            "282\n",
            "0.362109375\n",
            "283\n",
            "0.32265625\n",
            "284\n",
            "0.354296875\n",
            "285\n",
            "0.312109375\n",
            "286\n",
            "0.322265625\n",
            "287\n",
            "0.287890625\n",
            "288\n",
            "0.277734375\n",
            "289\n",
            "0.334375\n",
            "290\n",
            "0.2703125\n",
            "291\n",
            "0.3375\n",
            "292\n",
            "0.326953125\n",
            "293\n",
            "0.29765625\n",
            "294\n",
            "0.323046875\n",
            "295\n",
            "0.336328125\n",
            "296\n",
            "0.3234375\n",
            "297\n",
            "0.27265625\n",
            "298\n",
            "0.31015625\n",
            "299\n",
            "0.29921875\n",
            "300\n",
            "0.278125\n",
            "301\n",
            "0.33359375\n",
            "302\n",
            "0.277734375\n",
            "303\n",
            "0.33671875\n",
            "304\n",
            "0.312890625\n",
            "305\n",
            "0.28203125\n",
            "306\n",
            "0.286328125\n",
            "307\n",
            "0.311328125\n",
            "308\n",
            "0.322265625\n",
            "309\n",
            "0.284375\n",
            "310\n",
            "0.274609375\n",
            "311\n",
            "0.298046875\n",
            "312\n",
            "0.33515625\n",
            "313\n",
            "0.303125\n",
            "314\n",
            "0.274609375\n",
            "315\n",
            "0.295703125\n",
            "316\n",
            "0.307421875\n",
            "317\n",
            "0.270703125\n",
            "318\n",
            "0.291796875\n",
            "319\n",
            "0.310546875\n",
            "320\n",
            "0.2765625\n",
            "321\n",
            "0.287890625\n",
            "322\n",
            "0.30078125\n",
            "323\n",
            "0.276171875\n",
            "324\n",
            "0.273046875\n",
            "325\n",
            "0.328515625\n",
            "326\n",
            "0.27890625\n",
            "327\n",
            "0.271875\n",
            "328\n",
            "0.272265625\n",
            "329\n",
            "0.29609375\n",
            "330\n",
            "0.257421875\n",
            "331\n",
            "0.26875\n",
            "332\n",
            "0.2796875\n",
            "333\n",
            "0.254296875\n",
            "334\n",
            "0.2640625\n",
            "335\n",
            "0.278125\n",
            "336\n",
            "0.234765625\n",
            "337\n",
            "0.281640625\n",
            "338\n",
            "0.248046875\n",
            "339\n",
            "0.26953125\n",
            "340\n",
            "0.246484375\n",
            "341\n",
            "0.238671875\n",
            "342\n",
            "0.258203125\n",
            "343\n",
            "0.2234375\n",
            "344\n",
            "0.259375\n",
            "345\n",
            "0.286328125\n",
            "346\n",
            "0.284375\n",
            "347\n",
            "0.2484375\n",
            "348\n",
            "0.259765625\n",
            "349\n",
            "0.28203125\n",
            "350\n",
            "0.290234375\n",
            "351\n",
            "0.255859375\n",
            "352\n",
            "0.266015625\n",
            "353\n",
            "0.216796875\n",
            "354\n",
            "0.237890625\n",
            "355\n",
            "0.250390625\n",
            "356\n",
            "0.21875\n",
            "357\n",
            "0.19921875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "356"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSs0cMKvuSlc"
      },
      "source": [
        "### **The CSV file**\n",
        "The output_csv function is to output your sample data number to a standard format. Please pass in your beta0 and beta1 to generate the csv file. Notice that if your format is different from us, you'll get zero points in this homework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVTR73zRdnkC"
      },
      "source": [
        "import csv\n",
        "def output_csv(sample_num):\n",
        "  csv_file_name = student_id + '.csv'\n",
        "  with open(csv_file_name, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([sample_num])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZCLh04xuyMB"
      },
      "source": [
        "### **Sample Run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzJnqDxFuw2x"
      },
      "source": [
        "output_csv(sample_num)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}